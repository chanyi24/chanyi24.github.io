---
layout: single
title:  "회귀 알고리즘과 모델 규제"
---

## 2. 회귀 알고리즘과 모델 규제
### 2-1 머신러닝 용어
* 지도 학습 알고리즘
	* 분류(Classification): 클래스 중 하나로 분류하는 문제 (출력데이터 수: 유한개)
	* 회귀(Regression): 임의의 숫자 값을 예측하는 문제 (출력데이터 수: 무한개)            
<br>           

* KNN 알고리즘(지도학습의 하위개념)
	* K-최근접 이웃 분류 알고리즘: 예측하려는 샘플에 가장 가까운 샘플 K개를 선택 후, 샘플들의 클래스를 확인하여 다수 클래스를 새로운 샘플의 클래스로 예측한다.                  
![photo 34](/assets/img/blog/img34.png)               
	* K-최근접 이웃 회귀 알고리즘: 예측하려는 샘플에 가장 가까운 샘플 K개를 선택 후, 해당 K개의 샘플값의 평균을 구한다.                        
![photo 35](/assets/img/blog/img35.png)                      
<br>             

* 회귀 모델 성능 평가 척도: 타깃과 예측한 값 사이의 차이를 구한다.
* 결정계수(R^2)                        
![photo 36](/assets/img/blog/img36.png)                   
  * 타깃이 예측에 가까워지면 결정계수는 1에 가까운 값이 된다.
  * 타깃의 평균 정도를 예측한다면 결정계수는 0에 가까운 값이 된다. 
  * mean_absolute_error: 타깃과 예측의 평균 절댓값 오차
<br>           

* 과대적합(Overfitting): training set에만 적합한 모델로 training set에서의 성능은 매우 높고 test set의 성능은 매우 낮은 경우를 말한다.
* 과소적합(Underfitting): training set을 적절하게 학습하지 못하여 training set과 test set 모두 성능이 매우 낮거나 test set의 성능이 training set 보다 높은 경우를 말한다.
  * 또 다른 원인으로 training set과 test set의 크기가 매우 작아도 과소적합 현상이 나타날 수 있다.
* KNN 모델에서의 과소적합 sol: K값을 감소시켜 training set의 국지적인 패턴에 민감해지도록 한다.
* KNN 모델에서의 과대적합 sol: K값을 증가시켜 데이터 전반에 있는 일반적인 패턴을 따르도록 한다.
<br>

* 모델 파라미터: 머신러닝 모델이 특성에서 학습한 파라미터

### 2-2 농어 데이터와 KNN 회귀 알고리즘을 활용한 무게 예측
```python
# 데이터 준비
import numpy as np

# 농어 길이
perch_length = np.array(
    [8.4, 13.7, 15.0, 16.2, 17.4, 18.0, 18.7, 19.0, 19.6, 20.0,
     21.0, 21.0, 21.0, 21.3, 22.0, 22.0, 22.0, 22.0, 22.0, 22.5,
     22.5, 22.7, 23.0, 23.5, 24.0, 24.0, 24.6, 25.0, 25.6, 26.5,
     27.3, 27.5, 27.5, 27.5, 28.0, 28.7, 30.0, 32.8, 34.5, 35.0,
     36.5, 36.0, 37.0, 37.0, 39.0, 39.0, 39.0, 40.0, 40.0, 40.0,
     40.0, 42.0, 43.0, 43.0, 43.5, 44.0]
     )

# 농어 무
perch_weight = np.array(
    [5.9, 32.0, 40.0, 51.5, 70.0, 100.0, 78.0, 80.0, 85.0, 85.0,
     110.0, 115.0, 125.0, 130.0, 120.0, 120.0, 130.0, 135.0, 110.0,
     130.0, 150.0, 145.0, 150.0, 170.0, 225.0, 145.0, 188.0, 180.0,
     197.0, 218.0, 300.0, 260.0, 265.0, 250.0, 250.0, 300.0, 320.0,
     514.0, 556.0, 840.0, 685.0, 700.0, 700.0, 690.0, 900.0, 650.0,
     820.0, 850.0, 900.0, 1015.0, 820.0, 1100.0, 1000.0, 1100.0,
     1000.0, 1000.0]
     )

# data spliting
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    perch_length, perch_weight, random_state=42)

# KNN 회귀 모델 훈련
from sklearn.neighbors import KNeighborsRegressor

knr = KNeighborsRegressor()
knr.fit(train_input, train_target)

# KNN 회귀 모델 평가 1 (결정계수)
knr.score(test_input, test_target)

# KNN 회귀 모델 평가 2 (평균 절댓값 오차)
from sklearn.metrics import mean_absolute_error

test_prediction = knr.predict(test_input)
mae = mean_absolute_error(test_target, test_prediction)
```
* KNeighborsRegressor(): KNN 회귀 모델의 객체를 생성한다.
* score(): 회귀 모델에서의 score는 결정계수를 의미한다. (분류 모델: 정확도)
* mean_absolute_error(): test set에 대한 예측을 만든 후 test set과의 평균 절댓값 오차를 계산한다.

### 2-3 선형 회귀와 다항회귀

* KNN 회귀 모델의 단점                    
![photo 37](/assets/img/blog/img37.png)                          
=> 새로운 샘플이 훈련 세트의 범위를 벗어나면 엉뚱한 값을 예측할 수 있다. 해당 그래프에서는 농어의 길이가 아무리 늘어나더라도 무게는 증가하지 않는다.
<br>                         
        
KNN 회귀 모델의 한계를 보완하기 위한 다음과 같은 모델을 사용할 수 있다.
* 선형 회귀(Linear Regression): 특성과 타깃 사이의 관계를 가장 잘 나타내는 선형 방정식을 찾는 알고리즘.
<br>               

```python
# 선형 회귀 모델 생성 및 학습
from sklearn.linear_model import LinearRegression

lr = LinearRegression() # 선형 회귀 모델 객체 생성
lr.fit(train_input, train_target) # 선형 회귀 모델 학습

print(lr.coef_, lr.intercept_) # 선형 회귀 모델의 직선의 방정식 기울기, 절편
```
![photo 38](/assets/img/blog/img38.png)                                
* LinearRegression(): 선형 회귀 알고리즘 객체를 생성 후, fitting을 통해 모델을 학습한다.
  * fit_intercept 매개변수를 True로 지정하면 절편을 학습하지 않는다. (기본값 = True)
* lr.coef_: 학습된 선형 회귀 모델의 특성에 대한 계수(기울기)를 포함한 배열로 특성 개수와 동일하다.
* lr.intercept_: 학습된 선형 회귀 모델의 절편을 의미한다.             
<br>                  

* 다항 회귀(Polynomial Regression): 다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸다.
<br>                
                                    
```python
train_poly = np.column_stack((train_input ** 2, train_input))
test_poly = np.column_stack((test_input ** 2, test_input))

# 이하 과정은 선형 회귀와 동일
lr = LinearRegression()
lr.fit(train_poly, train_target)
```
![photo 39](/assets/img/blog/img39.png)                    
* input data 제곱항을 추가해 2차 방정식 형태로 학습할 수 있도록 한다.           
<br>                  

### 2-4 특성 공학과 다중 회귀

* 다중 회귀(Multiple Regression): 여러 개의 특성을 사용한 선형 회귀
![photo 40](/assets/img/blog/img40.png)             
* 1개의 특성 사용 시, 선형 회귀 모델은 직선을 학습한다.
* 2개의 특성 사용 시, 선형 회귀 모델은 평면을 학습한다.            
<br>               

* 특성 공학: 기존의 특성을 사용해 새로운 특성을 뽑아내는 작업
* 변환기(transformer): 특성을 만들거나 전처리하기 위한 클래스로 target data 없이 input data를 변환한다.
  * ex) fit(), transform()            
<br>                    

```python
# 변환기 활용한 다중 회귀 학습 모델
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=3, include_bias=False)
poly.fit(train_input)
train_poly = poly.transform(train_input)
test_poly = poly.transform(test_input)

# 다중 회귀 학습 모델
lr.fit(train_poly, train_target)
print(lr.score(train_poly, train_target))
```
* PolynomialFeatures(): 각 특성을 제곱한 항을 추가하고 특성끼리 서로 곱한 항을 추가한다.
* fit(): 새롭게 만들 특성 조합을 찾는다.
* transform(): 이후, 실제로 데이터를 변환한다.
* poly.get_feature_names() 메서드를 통해 특성 조합을 확인할 수 있다.
* degree 매개변수를 사용하여 필요한 고차항의 최대차수를 설정할 수 있다. (기본값 2)
* 고차원으로 설정 시, 샘플의 개수보다 특성의 개수가 크게 늘어날 수 있어 훈련 세트에 과대적합될 가능성이 높다.

### 2-5 릿지 회귀와 라쏘 회귀

* 규제(Regularization): 머신러닝 모델이 훈련 세트를 과도하게 학습하는 오버피팅을 방지하지 위해 규제하는 것.
  * 선형 회귀 모델에 규제를 적용하는 경우, 계수 값의 크기를 맞추기 위해 정규화 과정을 거친다.
  * 선형 회귀 모델의 경우, 특성에 곱해지는 계수(기울기)의 크기를 작게 만든다.            
![photo 41](/assets/img/blog/img41.png)                    
<br>                  

* 릿지 회귀(Ridge): 계수를 제곱한 값을 기준으로 규제를 적용한 선형 회귀 모델
* 라쏘 회귀(Lasso): 계수의 절댓값을 기준으로 규제를 적용한 선형 회귀 모델
  * 최적의 모델을 찾기 위해 좌표축을 따라 최적화를 수행하는 좌표 하강법을 사용한다.
* 공통점: 선형 회귀 모델에 규제를 추가한 모델
* 차이점: 두 알고리즘 모두 계수의 크기를 줄이지만, 라쏘는 0으로 만들 수 있다.
<br>                  

```python
# 규제를 적용하기 전 정규화 진행
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_poly)

train_scaled = ss.transform(train_poly)
test_scaled = ss.transform(test_poly)

# 릿지 회귀모델
from sklearn.linear_model import Ridge

ridge = Ridge()
ridge.fit(train_scaled, train_target)
print(ridge.score(train_scaled, train_target))
print(ridge.score(test_scaled, test_target))

# 라쏘 회귀 모델
from sklearn.linear_model import Lasso

lasso = Lasso()
lasso.fit(train_scaled, train_target)
print(lasso.score(train_scaled, train_target))
print(lasso.score(test_scaled, test_target))
```
* Ridge() : 릿지 회귀 모델 객체를 생성 후, fitting을 통해 릿지 회귀 모델을 생성한다.
* Lasso() : 라쏘 회귀 모델 객체를 생성 후, fitting을 통해 라쏘 회귀 모델을 생성한다.
* Ridge()와 Lasso()은 alpha 매개변수를 통해 규제의 강도를 조절할 수 있다.
  * alpta 값 증가 -> 규제의 강도가 세지므로 계수 값이 줄어 과소적합되도록 유도한다.
  * alpha 값 감소 -> 규제의 강도가 약해지므로 계수를 줄이는 역할이 줄어들고 과대적합될 가능성이 증가한다.
  * alpha와 같이 모델이 학습하는 값이 아닌 사람이 직접 지정해야 하는 파라미터를 하이퍼파라미터하고 부른다.
<br>                  

* 시각화를 통해 릿지, 라쏘 모델의 적절한 alpha값 찾기                   
<br>                              

```python
train_score_ridge = []
test_score_ridge = []

# alpha 값에 따라 달라지는 릿지 회귀 모델 성능 시각화
alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    ridge = Ridge(alpha=alpha) # 릿지 모델 생성
    ridge.fit(train_scaled, train_target) # 모델 학습
    # train set, test set에 대한 점수를 리스트에 추
    train_score_ridge.append(ridge.score(train_scaled, train_target))
    test_score_ridge.append(ridge.score(test_scaled, test_target)

plt.plot(np.log10(alpha_list), train_score_ridge)
plt.plot(np.log10(alpha_list), test_score_ridge)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()

# alpha 값에 따라 달라지는 라쏘 회귀 모델 성능 시각화
train_score_lasso = []
test_score_lasso = []

alpha_list = [0.001, 0.01, 0.1, 1, 10, 100]
for alpha in alpha_list:
    # 라쏘 모델을 만듭니다
    lasso = Lasso(alpha=alpha, max_iter=10000)
    # 라쏘 모델을 훈련합니다
    lasso.fit(train_scaled, train_target)
    # 훈련 점수와 테스트 점수를 저장합니다
    train_score_lasso.append(lasso.score(train_scaled, train_target))
    test_score_lasso.append(lasso.score(test_scaled, test_target))

plt.plot(np.log10(alpha_list), train_score_lasso)
plt.plot(np.log10(alpha_list), test_score_lasso)
plt.xlabel('alpha')
plt.ylabel('R^2')
plt.show()
```
![photo 42](/assets/img/blog/img42.png)  
![photo 43](/assets/img/blog/img43.png)                   
* 적절한 alpha 값은 두 그래프가 가장 가깝고 test set의 점수가 가장 높은 지점이다.
* 파란색: training set / 주황색: test set
* 라쏘 모델은 계수 값을 0으로 만들 수 있기 때문에 사용하지 않은 특성을 확인할 수 있다. (아래 코드 참조)
```python
print(np.sum(lasso.coef_ == 0))
```
<br>
