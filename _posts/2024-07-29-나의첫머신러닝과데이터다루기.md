---
layout: single
title:  "나의 첫 머신러닝 & 데이터 다루기"
---
## 1. 나의 첫 머신러닝 & 데이터 다루기
### 1-1 인공지능과 머신러닝, 딥러닝
* 인공지능(Artificial Intelligence): 사람처럼 학습하고 추론할 수 있는 지능을 가진 컴퓨터 시스템을 만드는 기술.
  * 강인공지능(Strong AI): 사람과 구분하기 어려운 지능을 가진 컴퓨터 시스템
  * 약인공지능(Weak AI): 특정 분야에서 사람의 일을 도와주는 보조 역할을 하는 인공지능.               
<br>

* 머신러닝: 데이터에서 규칙을 자동으로 학습하는 알고리즘을 연구하는 분야.
* 머신러닝 라이브러리: 사이킷런(scikit-learn)
* 사이킷런(scikit-learn) 특징
	* 파이썬 API를 사용하므로 사용하기 편리하고 컴파일하지 않아도 된다.
	* 안정적이고 성능이 검증된 머신러닝 알고리즘이 포함되어 있다.
<br>
                                 
* 딥러닝: 머신러닝 알고리즘 중 인공 신경망을 기반으로 한 방법들
* 딥러닝 라이브러리: 텐서플로(Tensorflow), 파이토치(PyTorch)
* 딥러닝 라이브러리 특징
	* 인공 신경망 알고리즘을 전문으로 다루고 있다.
	* 파이썬 API를 제공해 사용하기 쉽다.
                                      
### 1-2. 머신러닝 키워드
* 분류(Classification): 여러 개의 종류(Class) 중 하나를 구별해 내는 문제
  * 이진 분류(Binary Classification): 2개의 클래스 중 하나를 고르는 문제
  * 다중 분류(Multi Classification): 3개 이상의 클래스 중 하나를 고르는 문제
* 특성(feature): 데이터의 특징           
<br>
         
* 머신러닝 알고리즘
	* 지도 학습: input data와 target data(정답)으로 이루어진 훈련 데이터을 이용해 알고리즘이 target을 맞히는 것을 학습한다.
	* 비지도 학습: target data 없이 input 데이터만 사용하여 데이터의 특징 파악하고 변형하는데 도움을 준다.
	* 강화학습: target이 아닌 알고리즘이 행동한 결과로 얻은 보상을 사용해 학습한다.                   
<br>
                              
* 훈련 세트와 테스트 세트
머신러닝 알고리즘의 성능을 제대로 평가하기 위해서는 훈련 데이터와 평가에 사용할 데이터가 달라야 한다. 훈련에 사용한 데이터로 모델을 평가한다면 모델의 정확도가 매우 높아져 성능을 제대로 측정할 수 없다.
  * 훈련 세트(training set): 훈련에 사용되는 데이터, 훈련 세트로 fit 메서드를 사용해 모델 훈련
  * 테스트 세트(test set): 평가에 사용하는 데이터, socre() 메서드를 사용해 모델 평가                        
<br>

* 샘플(Sample): 하나의 데이터
* 샘플링 편향(Sampling bias): 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않아 샘플링이 한쪽으로 치우쳐진 것으로 특정 종류의 샘플이 과도하게 많은 샘플링 편향을 가지고 있다면 제대로 된 지도 학습 모델을 만들 수 없다.
           
<br>                                     
* 데이터 전처리: 머신러닝 모델에 훈련 데이터를 주입하기 전 가공하는 단계                

### 1-3 생선 데이터를 활용한 도미, 빙어 분류 문제
```python
# 도미, 빙어 데이터 준비
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0]
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
# 도미, 빙어 데이터 시각화
plt.scatter(bream_length, bream_weight)
plt.scatter(smelt_length, smelt_weight)
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![photo 30](/assets/img/blog/img30.png)           
* 데이터 시각화를 통해 도미와 빙어 데이터의 특징을 쉽게 파악할 수 있다.
* 산점도(Scatter Plot): x, y축으로 이뤄진 좌표계에 두 변수(x, y)의 관계를 표현하는 방법
  * 산점도 그래프가 일직선에 가까운 형태로 나타나는 경우를 선형(linear)적이라고 한다.                    
<br>              

```python
# numpy를 이용한 데이터 가공
import numpy as np
fish_data = np.column_stack((fish_length, fish_weight))
fish_target = np.concatenate((np.ones(35), np.zeros(14))
```
* np.column_stack(): 튜플로 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결한다.
* np.concatenate(): 차원을 따라 배열을 연결한다.
* np.ones(): 원하는 개수의 1을 채운 배열을 생성한다.
* np.zeros(): 원하는 개수의 0을 채운 배열을 생성한다.      
![photo 31](/assets/img/blog/img31.png)
* 머신러닝 알고리즘이 생선의 길이와 무게를 보고 도미와 빙어를 구분하는 규칙을 찾기 위해 어떤 생선이 도미인지 빙어인지 알려주어야 한다.
  * 머신러닝에서 2개를 구분하는 경우 찾으려는 대상을 1, 그 외에는 0으로 놓는다.            
<br>              

```python
# 사이킷런으로 훈련 세트와 테스트 세트 나누기
from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify=fish_target, random_state=42)
```
* train_test_split(): 전달되는 리스트나 배열을 비율에 맞게 섞어서 훈련 세트와 테스트 세트로 나누어 준다.
* fish_data: 입력 데이터(input_data)
* fish_target: 타겟 데이터(target data)
* random_state: 랜덤 시드 지정을 통해 동일한 분할 결과를 보장한다.
* stratify: 클래스 비율에 맞게 데이터를 나눈다.              
<br>            

```python
# KNeighborsClassifier 모델링
from sklearn.neighbors import KNeighborsClassifier # k-최근접 이웃 알고리즘 import
kn = KNeighborsClassifier(n_neighbors=5) # KNeighborsClassifier 클래스 객체 생성
kn.fit(fish_data, fish_target)
```
* K-최근접 이웃(K-Nearest Neighbors) 알고리즘: 주변의 다른 데이터(샘플) 중에서 다수를 차지하클래스를 예측으로 사용하는 알고리즘.
* K-Nearest Neighbors 특징
  * 새로운 데이터에 대해 예측할 때는 가장 가까운 직선거리에 어떤 데이터가 있는지 살핀다.
  * 단점: 데이터가 많은 경우 계산량이 많아 메모리를 많이 필요로 해 사용하기 어렵다.
  * n_neighbors 매개변수를 이용해 K의 개수 임의 지정 가능, 기본값 5             
<br>          

```python
kn.fit(fish_data, fish_target)
```
* fit(): 주어진 데이터로 알고리즘을 훈련시킨 뒤, 머신러닝 학습을 진행하여 모델 생성                         
<br>

```python
# Model 평가
kn.score(fish_data, fish_target)
``` 
* score(): 통해 모델 평가, 0과 1사이 값 반환(평가척도: 정확도)                
<br>            
 
### 1-4 도미, 빙어 분류 문제를 통한 데이터 전처리의 중요성
```python
# New Instance 예측 및 시각화
kn.predict([[25, 150]])
plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![photo 32](/assets/img/blog/img32.png)             
* predict(): 새로운 데이터의 정답을 예측
* 도미와 더 가까워 보이지만, 데이터 스케일의 차이로 new instance를 빙어로 잘못 예측(분류)
* Why? 두 특성의 스케일(Scale)이 다르기 때문이다.
* 거리 기반 알고리즘은 Scale에 매우 민감하므로 데이터 전처리 과정이 필요하다.            
<br>                   

```python
# 데이터 전처리(Scaling)
# 표준점수(Standard Score, Z-Score)
mean = np.mean(train_input, axis=0)
std = np.std(train_input, axis=0)
train_scaled = (train_input - mean) / std 
test_scaled = (test_input - mean) / std
```
* 표준점수(Standard Score, Z-Score): 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다.
* 표준점수를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다.
* np.mean(): 평균 계산
* np.std(): 표준편차 계산
* 동일한 기준으로 샘플을 변환하기 위해 테스트 세트도 훈련 세트의 mean, std를 이용해 변환해야 한다.           
<br>                     

```python
# 전처리 후 시각화
kn.score(test_scaled, test_target)
kn.predict([new])
new = ([25, 150] - mean) / std
distances, indexes = kn.kneighbors([new])
plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
```
![photo 33](/assets/img/blog/img33.png)             
* 스케일 변환을 통해 new instance를 올바르게 예측한 것을 알 수 있다.              
<br>

참고문헌: 혼자 공부하는 머신러닝+딥러닝 1,2장
